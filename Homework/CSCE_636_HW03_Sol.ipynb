{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VIavSB9FklF"
      },
      "source": [
        "# HOMEWORK 03\n",
        "\n",
        "**Submitted by: Diego Sol**\n",
        "\n",
        "CSCE 636-600: Deep Learning\n",
        "\n",
        "Professor: Dr. Anxiao Jiang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byUhW1l5FklN"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "**Check out the Jupyter notebook for Chapter 5 at https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter05_fundamentals-of-ml.ipynb . Answer the question: what regularization techniques were mentioned in that Jupyter notebook? (5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fllB0RugFklO"
      },
      "source": [
        "The Jupyter notebook for Chapter 5 mentions the following regularization techniques:\n",
        "- `L1 weight regularization`\n",
        "- `L2 weight regularization`\n",
        "- `Dropout regularization`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADuO2GbZFklP"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "**The MNIST dataset has 60,000 training images and 10,000 test images. Each image is a 28x28 array, where each array element is between 0 and 255. The images have 10 labels: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.**\n",
        "\n",
        "**We now create a new dataset of 30,000 training images, 5,000 test images and 5 labels (which are 0, 1, 2, 3, 4) as follows. First, randomly pair up the training images of label 0 with the training images of label 1, to get 6,000 such pairs. Then, for each pair (A,B) where A is an image of label 0 and B is an image of label 1, we create a new image of size 28x28, where each element's value is the \"average\" of the two corresponding pixel values in A and B. (So the new image is a \"mixture\" of the two original images.) This way we create 6,000 new \"mixture\" images for training. In a similar way, we create 1,000 new \"mixture\" images for testing. We give all these 6,000+1,000=7,000 new \"mixture\" images the new label 0. Then, in the same way, we create 6,000 new training images and 1,000 new test images by mixing the original images of label \"2\" and \"3\", and give them the new label 1; create 6,000 new training images and 1,000 new test images by mixing the original images of label \"4\" and \"5\", and give them the new label 2; create 6,000 new training images and 1,000 new test images by mixing the original images of label \"6\" and \"7\", and give them the new label 3; create 6,000 new training images and 1,000 new test images by mixing the original images of label \"8\" and \"9\", and give them the new label 4.**   \n",
        "\n",
        "**Your task: submit your code that creates the above new dataset; then for each of the 5 new labels, randomly select 2 images of that label from your new dataset, and display them in your submitted Jupyter notebook. (5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "HyUA-GgqI7qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "luY-LZihI7MI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and reformat image data"
      ],
      "metadata": {
        "id": "VtQZNoE9JLjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# resize and normalize image data\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "JXB0sP_SGXz_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new dataset and labels"
      ],
      "metadata": {
        "id": "f5yb3RC3JWVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_0 = np.asarray(np.where(train_labels==0))\n",
        "idx_1 = np.asarray(np.where(train_labels==1))\n",
        "idx_2 = np.asarray(np.where(train_labels==2))\n",
        "idx_3 = np.asarray(np.where(train_labels==3))\n",
        "idx_4 = np.asarray(np.where(train_labels==4))\n",
        "idx_5 = np.asarray(np.where(train_labels==5))\n",
        "idx_6 = np.asarray(np.where(train_labels==6))\n",
        "idx_7 = np.asarray(np.where(train_labels==7))\n",
        "idx_8 = np.asarray(np.where(train_labels==8))\n",
        "idx_9 = np.asarray(np.where(train_labels==9))"
      ],
      "metadata": {
        "id": "61B4jWTmJa9N"
      },
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "02de5609ef162b6bcc51dd429fd31b9a1f163c414526741362218fd7f2e354ad"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "CSCE_636_HW03_Sol.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}